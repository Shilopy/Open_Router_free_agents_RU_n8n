{
  "name": "OpenRouter_Free_Experts_RU",
  "nodes": [
    {
      "parameters": {},
      "id": "0e0bc4ff-494f-4115-898f-c692cdcf06b3",
      "name": "‚ñ∂Ô∏è Start Workflow",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -384,
        -208
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "api-key",
              "name": "openrouter_api_key",
              "value": "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",
              "type": "string"
            },
            {
              "id": "test-mode",
              "name": "test_mode",
              "value": "auto",
              "type": "string"
            },
            {
              "id": "custom-models",
              "name": "custom_models_list",
              "value": "",
              "type": "string"
            },
            {
              "id": "scoring-model",
              "name": "scoring_model",
              "value": "openrouter/aurora-alpha",
              "type": "string"
            },
            {
              "id": "max-models",
              "name": "max_models_to_test",
              "value": "15",
              "type": "number"
            },
            {
              "id": "rate-limit",
              "name": "rate_limit_ms",
              "value": "2000",
              "type": "number"
            }
          ]
        },
        "options": {}
      },
      "id": "4a4c636f-648c-4ace-bedc-d2de6ed2e73c",
      "name": "‚öôÔ∏è Configuration",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -320,
        256
      ]
    },
    {
      "parameters": {
        "jsCode": "// –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–∞—Å—Ç–æ–º–Ω—ã–π –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π)\nconst config = $input.first().json;\nconst testMode = config.test_mode || 'auto';\nconst customModels = config.custom_models_list || '';\nconst maxModels = parseInt(config.max_models_to_test) || 15;\n\nconsole.log('üîç Test Mode:', testMode);\nconsole.log('üìä Max models to test:', maxModels);\n\nif (testMode === 'custom' && customModels.trim()) {\n  // –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é\n  const modelsArray = customModels\n    .split(',')\n    .map(m => m.trim())\n    .filter(m => m)\n    .slice(0, maxModels);\n  \n  console.log(`‚úÖ Using custom model list (${modelsArray.length} models)`);\n  \n  return modelsArray.map((modelId, index) => ({\n    json: {\n      model_id: modelId,\n      source: 'custom_list',\n      priority: 'high',\n      original_index: index\n    }\n  }));\n} else {\n  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º ‚Äî –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –†–§ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (30 –≤–º–µ—Å—Ç–æ 15)\n  const defaultModels = [\n    'qwen/qwen-2.5-72b-instruct:free',\n    'qwen/qwen-110b-chat:free',\n    'deepseek/deepseek-chat',\n    'deepseek/deepseek-chat-v3.1',\n    'llama-3.1-70b-versatile',\n    'llama-3.1-8b-instant',\n    'upstage/solar-pro-3:free',\n    'nvidia/llama-3.1-nemotron-70b-instruct',\n    'arcee-ai/trinity-large-preview:free',\n    'qwen/qwen-2.5-coder-32b-instruct:free',\n    'deepseek/deepseek-coder',\n    'meta-llama/llama-3.1-70b-instruct:free',\n    'meta-llama/llama-3.2-3b-instruct:free',\n    'meta-llama/llama-3.2-1b-instruct:free',\n    'google/gemma-2-9b-it:free',\n    'google/gemma-2-27b-it',\n    'mistralai/mistral-7b-instruct:free',\n    'mistralai/mixtral-8x7b-instruct',\n    'openrouter/auto',\n    'upstage/solar-pro',\n    'qwen/qwen-2.5-7b-instruct:free',\n    'qwen/qwen-2.5-32b-instruct:free',\n    'deepseek/deepseek-math',\n    'mistralai/pixtral-12b:free',\n    'meta-llama/llama-3.3-70b-instruct',\n    'meta-llama/llama-3.3-70b-instruct:free',\n    'qwen/qwen-2.5-7b-chat:free',\n    'qwen/qwen-2.5-14b-instruct:free',\n    'qwen/qwen-2.5-14b-chat:free',\n    'qwen/qwen-2.5-7b-chat'\n  ];\n  \n  const modelsToTest = defaultModels.slice(0, maxModels);\n  console.log(`‚úÖ Using default model list for Russia (${modelsToTest.length} models)`);\n  \n  return modelsToTest.map((modelId, index) => ({\n    json: {\n      model_id: modelId,\n      source: 'default_russia_list',\n      priority: index < 6 ? 'high' : 'medium',\n      region: 'russia_compatible',\n      original_index: index\n    }\n  }));\n}"
      },
      "id": "88c22384-e56a-4ffd-a896-82f5cb61bbee",
      "name": "üìã Model Selection Logic",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -128,
        -32
      ]
    },
    {
      "parameters": {
        "url": "https://openrouter.ai/api/v1/models",
        "options": {}
      },
      "id": "83aaedbd-5b0d-42df-918b-d8dea8736655",
      "name": "üì° Fetch OpenRouter Catalog",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        -144,
        592
      ]
    },
    {
      "parameters": {
        "jsCode": "// ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ö–û–î ‚Äî —Ä–∞–∑—É–º–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –†–§ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–∞ –º–æ–¥–µ–ª–µ–π\nconst allModels = $input.first().json.data || [];\nconst maxModels = 30;\n\nconsole.log(`üì° Total models in catalog: ${allModels.length}`);\n\n// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\nlet stats = {\n  total: allModels.length,\n  free_by_tag: 0,\n  free_by_price: 0,\n  compatible: 0,\n  blocked: 0,\n  passed: 0\n};\n\nconst freeModels = allModels.filter(m => {\n  const id = (m.id || '').toLowerCase();\n  const pricing = m.pricing || {};\n  \n  // üîë –ö–†–ò–¢–ï–†–ò–ô 1: –ë–µ—Å–ø–ª–∞—Ç–Ω–æ—Å—Ç—å (–≥–∏–±–∫–∏–π –ø–æ–¥—Ö–æ–¥)\n  // –í–∞—Ä–∏–∞–Ω—Ç –ê: —è–≤–Ω–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø–æ —Ç–µ–≥—É\n  const isFreeByTag = \n    id.includes(':free') || \n    id.includes('/free') || \n    id.includes('preview') || \n    id.includes('demo') || \n    id.includes('trial') ||\n    id.includes('openrouter/free');\n  \n  // –í–∞—Ä–∏–∞–Ω—Ç –ë: \"–ø–æ—á—Ç–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ\" –ø–æ —Ü–µ–Ω–µ (–¥–æ $1.5 –∑–∞ 1–ú —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî —Ä–∞–∑—É–º–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –†–§)\n  const promptPrice = parseFloat(pricing.prompt) || 0;\n  const completionPrice = parseFloat(pricing.completion) || 0;\n  const isFreeByPrice = \n    (promptPrice <= 0.0000015 && completionPrice <= 0.000003); // $1.5/$3 –∑–∞ 1–ú —Ç–æ–∫–µ–Ω–æ–≤\n  \n  const isFree = isFreeByTag || isFreeByPrice;\n  \n  if (isFreeByTag) stats.free_by_tag++;\n  if (isFreeByPrice && !isFreeByTag) stats.free_by_price++;\n  \n  // üîë –ö–†–ò–¢–ï–†–ò–ô 2: –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –†–§ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)\n  // –†–∞–∑—Ä–µ—à–∞–µ–º –õ–Æ–ë–´–ï –º–æ–¥–µ–ª–∏, –ö–†–û–ú–ï —è–≤–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –¥–ª—è –†–§\n  const isBlockedForRussia = \n    id.includes('claude-3') ||      // Anthropic ‚Äî —á–∞—Å—Ç–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –†–§\n    id.includes('gpt-4o') ||        // GPT-4o ‚Äî —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è\n    id.includes('gpt-4-turbo') ||   // GPT-4 Turbo ‚Äî —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è\n    id.includes('gemini-1.5') ||    // Gemini 1.5 ‚Äî —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è\n    id.includes('o1-mini') ||       // OpenAI O1 ‚Äî —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è\n    id.includes('o1-preview') ||    // OpenAI O1 ‚Äî —á–∞—Å—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è\n    id.includes('gpt-3.5-turbo-0125'); // –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è, —á–∞—Å—Ç–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n  \n  const isCompatible = !isBlockedForRussia;\n  \n  if (isCompatible) stats.compatible++;\n  if (isBlockedForRussia) stats.blocked++;\n  \n  // üîë –ö–†–ò–¢–ï–†–ò–ô 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ—Ç—Å–µ–∫–∞–µ–º —Å–æ–≤—Å–µ–º —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏)\n  const hasEnoughContext = (m.context_length || 0) >= 4096;\n  \n  // ‚úÖ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –º–æ–¥–µ–ª—å –ø—Ä–æ—Ö–æ–¥–∏—Ç –µ—Å–ª–∏:\n  // 1. –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –ò–õ–ò –ø–æ—á—Ç–∏ –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è\n  // 2. –ù–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –†–§\n  // 3. –ò–º–µ–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n  const passes = isFree && isCompatible && hasEnoughContext;\n  \n  if (passes) stats.passed++;\n  \n  return passes;\n})\n.sort((a, b) => {\n  // –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ—Å—Ç–∏ (—Ç–µ–≥ :free –≤—ã—à–µ), –ø–æ—Ç–æ–º –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É\n  const aIsFreeTag = (a.id || '').toLowerCase().includes(':free');\n  const bIsFreeTag = (b.id || '').toLowerCase().includes(':free');\n  if (aIsFreeTag !== bIsFreeTag) return aIsFreeTag ? -1 : 1;\n  return (b.context_length || 0) - (a.context_length || 0);\n})\n.slice(0, maxModels);\n\n// –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ ‚Äî –ü–û–ö–ê–ó–´–í–ê–ï–¢ –ü–û–ß–ï–ú–£ –ú–û–î–ï–õ–ò –û–¢–ë–†–ê–°–´–í–ê–Æ–¢–°–Ø\nconsole.log('\\nüìä –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π:');\nconsole.log(`   –í—Å–µ–≥–æ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: ${stats.total}`);\nconsole.log(`   ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø–æ —Ç–µ–≥—É (:free): ${stats.free_by_tag}`);\nconsole.log(`   üí∞ –ü–æ—á—Ç–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø–æ —Ü–µ–Ω–µ (‚â§$1.5/1M): ${stats.free_by_price}`);\nconsole.log(`   üåç –°–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å –†–§: ${stats.compatible}`);\nconsole.log(`   üö´ –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –†–§: ${stats.blocked}`);\nconsole.log(`   ‚úÖ –ü—Ä–æ—à–ª–∏ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã: ${stats.passed}`);\nconsole.log(`   üì¶ –û—Ç–æ–±—Ä–∞–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: ${freeModels.length}\\n`);\n\nfreeModels.forEach((m, i) => {\n  const pricing = m.pricing || {};\n  const promptPrice = parseFloat(pricing.prompt) || 0;\n  const completionPrice = parseFloat(pricing.completion) || 0;\n  const isFreeTag = m.id.toLowerCase().includes(':free') ? 'üÜì' : 'üí∞';\n  \n  console.log(`${(i+1).toString().padStart(2)}. ${isFreeTag} ${m.id.padEnd(55)} | Context: ${(m.context_length || 'N/A').toString().padStart(6)} | Price: $${(promptPrice * 1000000).toFixed(2)}/$${(completionPrice * 1000000).toFixed(2)} per 1M tokens`);\n});\n\nreturn freeModels.map(m => ({\n  json: {\n    model_id: m.id,\n    context_length: m.context_length,\n    description: (m.description || '').substring(0, 100),\n    source: 'openrouter_catalog',\n    pricing: m.pricing || {},\n    architecture: m.architecture?.modality || 'text->text'\n  }\n}));"
      },
      "id": "4221461f-a312-4dbe-91af-8ed79839659f",
      "name": "üîç Filter Free Compatible Models",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -16,
        432
      ]
    },
    {
      "parameters": {},
      "id": "d6671101-382b-4ae7-9d75-e0cc0d96da8a",
      "name": "üîÄ Merge Model Lists",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [
        128,
        112
      ]
    },
    {
      "parameters": {
        "jsCode": "// –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–µ—Ä–≤—ã—Ö –º–µ—Å—Ç–∞—Ö (–∫–∞—Å—Ç–æ–º–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\nconst items = $input.all();\nconst uniqueModels = new Map();\n\nitems.forEach(item => {\n  const modelId = item.json.model_id;\n  \n  // –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º (–∫–∞—Å—Ç–æ–º–Ω—ã–π > –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π)\n  if (!uniqueModels.has(modelId)) {\n    uniqueModels.set(modelId, {\n      model_id: modelId,\n      context_length: item.json.context_length || 0,\n      description: item.json.description || 'N/A',\n      source: item.json.source || 'unknown',\n      priority: item.json.priority || 'medium',\n      pricing: item.json.pricing || {}\n    });\n  }\n});\n\nconst modelsArray = Array.from(uniqueModels.values());\n\nconsole.log(`\\n‚úÖ Final model list for testing: ${modelsArray.length} models\\n`);\nmodelsArray.forEach((m, i) => {\n  console.log(`${i+1}. ${m.model_id}`);\n  console.log(`   Source: ${m.source} | Priority: ${m.priority}`);\n  if (m.context_length) console.log(`   Context: ${m.context_length} tokens`);\n  console.log('---');\n});\n\nreturn modelsArray.map(m => ({ json: m }));"
      },
      "id": "4d52b1e6-d914-4b55-a34d-4eb37e10efcb",
      "name": "‚úÖ Deduplicate & Finalize List",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        240,
        512
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "4db7bb83-0b53-42cc-8116-43c34bcb5441",
      "name": "üì¶ Split In Batches (1 item)",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        352,
        64
      ]
    },
    {
      "parameters": {
        "jsCode": "// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ + –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è rate limiting\nconst delay = ms => new Promise(resolve => setTimeout(resolve, ms));\n\n// –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é –∏–∑ —É–∑–ª–∞ Configuration\nconst config = $('‚öôÔ∏è Configuration').first().json;\nconst rateLimit = config.rate_limit_ms || 2000;\nconst apiKey = config.openrouter_api_key;\n\nawait delay(rateLimit); // –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è 429 –æ—à–∏–±–æ–∫\n\nconst modelData = $input.first().json;\nconst modelId = modelData.model_id;\n\nconsole.log(`\\n‚è≥ Initializing test for model: ${modelId}`);\nconsole.log(`‚è±Ô∏è Rate limiting: ${rateLimit}ms`);\nconsole.log(`üîë API Key: ${apiKey ? '***' + apiKey.slice(-4) : 'NOT SET!'}`);\n\nif (!apiKey) {\n  throw new Error('OpenRouter API key is not set! Please configure openrouter_api_key in the Configuration node.');\n}\n\nreturn [{\n  json: {\n    model_id: modelId,\n    context_length: modelData.context_length || 0,\n    description: modelData.description || 'N/A',\n    source: modelData.source || 'unknown',\n    priority: modelData.priority || 'medium',\n    pricing: modelData.pricing || {},\n    openrouter_api_key: apiKey,\n    started_at: Date.now(),\n    test_session_id: `${modelId}_${Date.now()}`,\n    test_results: {}\n  }\n}];"
      },
      "id": "a33664ea-dec1-4b13-bc77-ad756e2e6fff",
      "name": "üöÄ Init Model Context",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        464,
        272
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'Bearer ' + $json.openrouter_api_key }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\"model\":\"{{$json.model_id}}\",\"messages\":[{\"role\":\"user\",\"content\":\"Respond with exactly: 'OK'\"}],\"max_tokens\":5,\"temperature\":0.1}",
        "options": {
          "timeout": 20000
        }
      },
      "id": "25a6749f-1670-4636-840b-56df276a77f4",
      "name": "üß™ Test 1: Availability",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        592,
        528
      ],
      "continueOnFail": true
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'Bearer ' + $json.openrouter_api_key }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\"model\":\"{{$json.model_id}}\",\"messages\":[{\"role\":\"user\",\"content\":\"Analyze the following Python code for security vulnerabilities and suggest improvements. Be concise. Code: `import os; password = \\\"admin123\\\"; os.system(f'echo {password}')`\"}],\"max_tokens\":300}",
        "options": {
          "timeout": 45000
        }
      },
      "id": "2522599d-dc3b-41a9-be95-f303b1197633",
      "name": "üõ°Ô∏è Test 2: Security Analysis",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        848,
        528
      ],
      "continueOnFail": true
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'Bearer ' + $json.openrouter_api_key }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\"model\":\"{{$json.model_id}}\",\"messages\":[{\"role\":\"user\",\"content\":\"Refactor this simple Python function to be more robust and production-ready. Add type hints, a docstring, and basic error handling. Code: `def add(a, b): return a + b`\"}],\"max_tokens\":400}",
        "options": {
          "timeout": 45000
        }
      },
      "id": "3cf58eb7-a5d2-4687-a80c-d046b0a6881c",
      "name": "‚ú® Test 3: Code Quality",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        1136,
        528
      ],
      "continueOnFail": true
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'Bearer ' + $json.openrouter_api_key }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\"model\":\"{{$json.model_id}}\",\"messages\":[{\"role\":\"user\",\"content\":\"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? Explain your reasoning step-by-step.\"}],\"max_tokens\":500}",
        "options": {
          "timeout": 45000
        }
      },
      "id": "4be6f7cd-1b9f-4455-9c00-ddb2d67c6564",
      "name": "üß† Test 4: Logical Reasoning",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        1360,
        528
      ],
      "continueOnFail": true
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'Bearer ' + $json.openrouter_api_key }}"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\"model\":\"{{$json.model_id}}\",\"messages\":[{\"role\":\"user\",\"content\":\"Generate a professional Python docstring for the function `def process_data(data, *, threshold=0.5, normalize=True):`. Include sections for a brief summary, arguments (Args), return value (Returns), and potential errors (Raises).\"}],\"max_tokens\":400}",
        "options": {
          "timeout": 45000
        }
      },
      "id": "de5d4102-21a2-4605-90e4-9c70bb341d31",
      "name": "üìö Test 5: Documentation",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.4,
      "position": [
        1584,
        528
      ],
      "continueOnFail": true
    },
    {
      "parameters": {
        "jsCode": "// ‚úÖ –ö–û–†–†–ï–ö–¢–ù–ê–Ø –ê–ì–†–ï–ì–ê–¶–ò–Ø –° –ì–†–£–ü–ü–ò–†–û–í–ö–û–ô –ü–û –ú–û–î–ï–õ–ò (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\nconst items = $input.all();\nconst modelResults = new Map();\n\n// –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏: {–º–æ–¥–µ–ª—å} + {—Ç–∏–ø —Ç–µ—Å—Ç–∞}\nitems.forEach(item => {\n  const modelId = item.json.model_id?.trim();\n  \n  if (!modelId) {\n    console.warn('‚ö†Ô∏è Skipping item without model_id:', item.json);\n    return;\n  }\n  \n  // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)\n  if (!modelResults.has(modelId)) {\n    modelResults.set(modelId, {\n      model_id: modelId,\n      context_length: item.json.context_length || 0,\n      description: item.json.description || 'N/A',\n      source: item.json.source || 'unknown',\n      priority: item.json.priority || 'medium',\n      scores: {},\n      responses: {},\n      response_times: {},\n      costs: {},\n      errors: []\n    });\n  }\n  \n  const modelData = modelResults.get(modelId);\n  const tests = item.json.test_results || {};\n\n  // –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Ç–µ—Å—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\n  for (const testType in tests) {\n    const result = tests[testType];\n\n    // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞ —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ (1-10)\n    if (result.success && result.response) {\n      let score = 5; // –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å)\n      const response = (result.response || '').toLowerCase();\n      \n      // –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ç–µ—Å—Ç–∞:\n      switch(testType) {\n        case 'availability':\n          // –¢–µ—Å—Ç 1: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ‚Äî –æ–∂–∏–¥–∞–µ–º —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç \"OK\"\n          score = response.trim() === 'ok' ? 10 : (response.includes('ok') ? 8 : 3);\n          break;\n        case 'security':\n          // –¢–µ—Å—Ç 2: –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ‚Äî –∏—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫–æ–¥–µ (–∂–µ—Å—Ç–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)\n          score = response.includes('vulnerability') || response.includes('security') || response.includes('insecure') || response.includes('hardcoded') || response.includes('password') ? 9 : \n                  response.length > 150 ? 8 : \n                  response.length > 50 ? 6 : 3;\n          break;\n        case 'code_quality':\n          // –¢–µ—Å—Ç 3: –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ (–∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: def, return, try, except, raise)\n          score = response.includes('def ') && response.includes('return') ? 9 : \n                  response.includes('pep8') || response.includes('error') || response.includes('try') || response.includes('except') ? 8 : 5;\n          break;\n        case 'reasoning':\n          // –¢–µ—Å—Ç 4: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ (–∏—â–µ–º —Å–ª–æ–≤–∞: step, therefore, conclusion, logic, reasoning)\n          score = response.includes('step') && response.includes('therefore') ? 9 : \n                  response.length > 200 ? 8 : 5;\n          break;\n        case 'documentation':\n          // –¢–µ—Å—Ç 5: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—Å—Ç—Ä–∏–Ω–≥–∞ —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏ Args, Returns, Raises (–∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: Args, Returns, Raises, docstring)\n          score = response.includes('args:') && response.includes('returns:') ? 10 : \n                  response.includes('docstring') || response.includes('raises:') ? 8 : 5;\n          break;\n      }\n      \n      // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ—Ç–ª–∞–¥–∫–∏:\n      modelData.scores[testType] = score;\n      modelData.responses[testType] = (result.response || '').substring(0, 300);\n      modelData.response_times[testType] = result.latency || 0;\n      modelData.costs[testType] = result.cost || 0;\n    } else {\n      // –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n      modelData.scores[testType] = 0;\n      modelData.errors.push(`${testType}: status=${result.success}, latency=${result.latency}ms`);\n    }\n  }\n});\n\n// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∞—Å—Å–∏–≤ –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º—É —Å–∫–æ—Ä—É (—É—á–∏—Ç—ã–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ, —É—Å–ø–µ—Ö–∏, –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å)\nconst results = Array.from(modelResults.values()).map(model => {\n  const scores = Object.values(model.scores).filter(s => s > 0);\n  const avgScore = scores.length ? (scores.reduce((a,b) => a+b, 0) / scores.length) : 0;\n  const successCount = Object.values(model.scores).filter(s => s > 0).length;\n  const totalCost = Object.values(model.costs).reduce((a,b) => a+b, 0);\n  const avgLatency = Object.values(model.response_times).reduce((a,b) => a+b, 0) / (Object.keys(model.response_times).length || 1);\n  \n  // –†–∞—Å—á–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤:\n  // –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä = —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ (1-10)\n  // –®—Ç—Ä–∞—Ñ –∑–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: -0.1 –∑–∞ –∫–∞–∂–¥—ã–µ 1000 –º—Å —Å–≤–µ—Ä—Ö 5 —Å–µ–∫—É–Ω–¥ (–º–∞–∫—Å–∏–º—É–º -3 –±–∞–ª–ª–∞)\n  // –®—Ç—Ä–∞—Ñ –∑–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å: -0.5 –∑–∞ –∫–∞–∂–¥—ã–µ $0.001 —Å–≤–µ—Ä—Ö $0.005 (–º–∞–∫—Å–∏–º—É–º -2 –±–∞–ª–ª–∞)\n  // –ë–æ–Ω—É—Å –∑–∞ 100% —É—Å–ø–µ—Ö: +1 –±–∞–ª–ª –µ—Å–ª–∏ –≤—Å–µ 5 —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ (5/5)\n  \n  const latencyPenalty = Math.min(Math.max(0, (avgLatency - 5000) / 10000), 3); // –ú–∞–∫—Å -3 –∑–∞ –º–µ–¥–ª–µ–Ω–Ω–æ—Å—Ç—å (>15 —Å–µ–∫)\n  const costPenalty = Math.min(totalCost * 200, 2); // –ú–∞–∫—Å -2 –∑–∞ –¥–æ—Ä–æ–≥–æ–≤–∏–∑–Ω—É (> $0.01)\n  const successBonus = successCount === 5 ? 1 : 0; // +1 –∑–∞ –∏–¥–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n  \n  const overallScore = Math.max(0, avgScore - latencyPenalty - costPenalty + successBonus);\n  \n  return {\n    model_id: model.model_id,\n    context_length: model.context_length,\n    description: model.description,\n    source: model.source,\n    priority: model.priority,\n    overall_score: parseFloat(overallScore.toFixed(2)),\n    quality_score: parseFloat(avgScore.toFixed(2)),\n    success_rate: `${successCount}/5`,\n    avg_latency: parseFloat(avgLatency.toFixed(0)),\n    total_cost: parseFloat(totalCost.toFixed(6)),\n    latency_penalty: parseFloat(latencyPenalty.toFixed(2)),\n    cost_penalty: parseFloat(costPenalty.toFixed(2)),\n    success_bonus: successBonus,\n    scores: model.scores,\n    response_times: model.response_times,\n    costs: model.costs,\n    errors: model.errors,\n    tested_at: new Date().toISOString()\n  };\n}).sort((a, b) => {\n  // –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –æ–±—â–µ–º—É —Å–∫–æ—Ä—É, –ø–æ—Ç–æ–º –ø–æ —É—Å–ø–µ—Ö–∞–º, –ø–æ—Ç–æ–º –ø–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –ø–æ—Ç–æ–º –ø–æ —Ü–µ–Ω–µ, –ø–æ—Ç–æ–º –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º —Å–∫–æ—Ä–æ–º)\n  const successA = parseInt(a.success_rate);\n  const successB = parseInt(b.success_rate);\n  \n  if (b.overall_score !== a.overall_score) return b.overall_score - a.overall_score;\n  if (successB !== successA) return successB - successA;\n  if (a.avg_latency !== b.avg_latency) return a.avg_latency - b.avg_latency;\n  if (a.total_cost !== b.total_cost) return a.total_cost - b.total_cost;\n  return (b.context_length || 0) - (a.context_length || 0);\n});\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:\nconsole.log(`\\n‚úÖ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ ${items.length} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è ${results.length} –º–æ–¥–µ–ª–µ–π\\n`);\nresults.forEach((r, i) => {\n  const successCount = parseInt(r.success_rate);\n  const statusEmoji = successCount === 5 ? '‚úÖ' : successCount >= 3 ? '‚ö†Ô∏è' : '‚ùå';\n  \n  console.log(`${(i+1).toString().padStart(2)}. ${statusEmoji} ${r.model_id}`);\n  console.log(`   Overall: ${r.overall_score.toFixed(2)}/10 | Quality: ${r.quality_score.toFixed(2)}/10 | Success: ${r.success_rate}`);\n  console.log(`   Latency: ${r.avg_latency}ms (-${r.latency_penalty.toFixed(2)}) | Cost: $${r.total_cost} (-${r.cost_penalty.toFixed(2)})`);\n  console.log(`   Security: ${r.scores.security || 0}/10 | Code: ${r.scores.code_quality || 0}/10 | Reasoning: ${r.scores.reasoning || 0}/10 | Docs: ${r.scores.documentation || 0}/10`);\n  if (r.errors.length > 0) console.log(`   ‚ö†Ô∏è Errors: ${r.errors.length}`);\n  console.log('---');\n});\n\nreturn results.map(r => ({ json: r }));"
      },
      "id": "4f66c722-b1c5-4dd0-98c1-aac064088746",
      "name": "üìä Aggregate & Rank Results (FIXED)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2032,
        304
      ]
    },
    {
      "parameters": {
        "jsCode": "// –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–æ–ª—è–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏)\nconst models = $input.all().map(item => item.json);\n\nconsole.log('\\n' + '='.repeat(80));\nconsole.log('ü§ñ AI MODEL SCOUT ‚Äî FINAL REPORT');\nconsole.log('='.repeat(80));\nconsole.log(`üìÖ Generated: ${new Date().toLocaleString('ru-RU')}`);\nconsole.log(`üìä Models Tested: ${models.length}`);\nconsole.log(`‚úÖ Fully Working (5/5): ${models.filter(m => m.success_rate === '5/5').length}`);\nconsole.log(`‚ö†Ô∏è Partial Issues (3-4/5): ${models.filter(m => m.success_rate !== '5/5' && m.success_rate !== '0/5' && parseInt(m.success_rate) >= 3).length}`);\nconsole.log(`‚ùå Not Working (0-2/5): ${models.filter(m => parseInt(m.success_rate) < 3).length}`);\nconsole.log();\n\nconsole.log('üèÜ TOP 10 RECOMMENDED MODELS:\\n');\nmodels.slice(0, 10).forEach((m, i) => {\n  const successCount = parseInt(m.success_rate);\n  const statusEmoji = successCount === 5 ? '‚úÖ' : successCount >= 3 ? '‚ö†Ô∏è' : '‚ùå';\n  \n  console.log(`${i+1}. ${statusEmoji} ${m.model_id}`);\n  console.log(`   Overall Score: ${m.overall_score.toFixed(2)}/10 | Quality: ${m.quality_score.toFixed(2)}/10 | Success: ${m.success_rate}`);\n  console.log(`   Context Length: ${m.context_length || 'N/A'} tokens | Source: ${m.source}`);\n  console.log(`   Latency: ${m.avg_latency}ms | Cost: $${m.total_cost}`);\n  console.log(`   Security: ${m.scores.security || 0}/10 | Code: ${m.scores.code_quality || 0}/10 | Reasoning: ${m.scores.reasoning || 0}/10 | Docs: ${m.scores.documentation || 0}/10`);\n  \n  // –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —Ä–æ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ—Ä–æ–≤ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö)\n  let bestFor = [];\n  if (m.scores.security >= 8) bestFor.push('Security Expert');\n  if (m.scores.code_quality >= 8) bestFor.push('Code Quality Specialist');\n  if (m.scores.reasoning >= 8) bestFor.push('Reasoning Specialist');\n  if (m.scores.documentation >= 8) bestFor.push('Documentation Expert');\n  \n  if (bestFor.length > 0) {\n    console.log(`   üí° Best For: ${bestFor.join(', ')}`);\n  }\n  \n  if (m.errors.length > 0) {\n    console.log(`   ‚ö†Ô∏è Errors: ${m.errors.join('; ').substring(0, 100)}...`);\n  }\n  \n  console.log();\n});\n\nconsole.log('üí° ORCHESTRATION ROLE RECOMMENDATIONS:\\n');\n\n// –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∫–æ—Ä–æ–≤ (–¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º)\n// –£–î–ê–õ–ï–ù–û: type RoleRecommendation = { ... };\n\nconst roles = [];  // –ë–´–õ–û: const roles: RoleRecommendation[] = [];\n\n// Primary Orchestrator - —Å–∞–º–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (5/5 —É—Å–ø–µ—Ö–æ–≤ + –≤—ã—Å–æ–∫–∏–π –æ–±—â–∏–π —Å–∫–æ—Ä) –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤)\nconst orchestrators = models.filter(m => m.success_rate === '5/5').sort((a, b) => b.overall_score - a.overall_score);\nif (orchestrators.length > 0) {\n  roles.push({\n    role: 'Primary Orchestrator',\n    model_id: orchestrators[0].model_id,\n    score: orchestrators[0].overall_score,\n    notes: 'Best for coordinating multi-agent workflows and complex orchestration tasks'\n  });\n}\n\n// Security Expert - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –∏ –∞—É–¥–∏—Ç–∞ –∫–æ–¥–∞)\nconst securityModels = models.filter(m => m.scores.security > 0).sort((a, b) => (b.scores.security || 0) - (a.scores.security || 0));\nif (securityModels.length > 0 && securityModels[0].scores.security >= 7) {\n  roles.push({\n    role: 'Security Expert',\n    model_id: securityModels[0].model_id,\n    score: securityModels[0].scores.security,\n    notes: 'Vulnerability detection and security analysis for code auditing'\n  });\n}\n\n// Code Quality Specialist - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∫–æ–¥–∞ (–¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–¥–∞)\nconst codeModels = models.filter(m => m.scores.code_quality > 0).sort((a, b) => (b.scores.code_quality || 0) - (a.scores.code_quality || 0));\nif (codeModels.length > 0 && codeModels[0].scores.code_quality >= 7) {\n  roles.push({\n    role: 'Code Quality Specialist',\n    model_id: codeModels[0].model_id,\n    score: codeModels[0].scores.code_quality,\n    notes: 'Code improvement, refactoring, PEP8 compliance and error handling'\n  });\n}\n\n// Reasoning Specialist - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –ø–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á)\nconst reasoningModels = models.filter(m => m.scores.reasoning > 0).sort((a, b) => (b.scores.reasoning || 0) - (a.scores.reasoning || 0));\nif (reasoningModels.length > 0 && reasoningModels[0].scores.reasoning >= 7) {\n  roles.push({\n    role: 'Reasoning Specialist',\n    model_id: reasoningModels[0].model_id,\n    score: reasoningModels[0].scores.reasoning,\n    notes: 'Complex logical reasoning and problem-solving for analytical tasks'\n  });\n}\n\n// Documentation Expert - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—Å—Ç—Ä–∏–Ω–≥–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)\nconst docsModels = models.filter(m => m.scores.documentation > 0).sort((a, b) => (b.scores.documentation || 0) - (a.scores.documentation || 0));\nif (docsModels.length > 0 && docsModels[0].scores.documentation >= 7) {\n  roles.push({\n    role: 'Documentation Expert',\n    model_id: docsModels[0].model_id,\n    score: docsModels[0].scores.documentation,\n    notes: 'Professional documentation and docstring generation for codebases'\n  });\n}\n\nroles.forEach(role => {\n  console.log(`${role.role.padEnd(25)} ‚Üí ${role.model_id} (${role.score.toFixed(1)}/10)`);\n  console.log(`   ${role.notes}`);\n});\n\nconsole.log('\\n' + '='.repeat(80));\nconsole.log('‚úÖ Workflow completed successfully');\nconsole.log('='.repeat(80) + '\\n');\n\nconsole.log('üí° USAGE INSTRUCTIONS:');\nconsole.log('   - To test custom models: Set test_mode=\"custom\" and provide comma-separated model IDs in custom_models_list');\nconsole.log('   - To use default Russia-compatible list: Set test_mode=\"auto\" or leave empty');\nconsole.log('   - Adjust rate_limit_ms to control delay between model tests (default: 2000ms)');\nconsole.log('   - Results are sorted by overall score with penalties for latency and cost');\nconsole.log();\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö/—Ñ–∞–π–ª:\nreturn [{\n  json: {\n    report_generated_at: new Date().toISOString(),\n    models_tested: models.length,\n    models_working: models.filter(m => m.success_rate === '5/5').length,\n    models_partial: models.filter(m => m.success_rate !== '5/5' && m.success_rate !== '0/5').length,\n    models_failed: models.filter(m => m.success_rate === '0/5').length,\n    ranked_models: models,\n    role_recommendations: roles.map(r => ({\n      role: r.role,\n      model_id: r.model_id,\n      score: r.score,\n      notes: r.notes\n    })),\n    top_model: models.length > 0 ? {\n      model_id: models[0].model_id,\n      overall_score: models[0].overall_score,\n      success_rate: models[0].success_rate\n    } : null\n  }\n}];"
      },
      "id": "fb01980a-1d13-4722-87c5-24acd7c56147",
      "name": "‚úÖ Final Report & Recommendations",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1824,
        576
      ]
    },
    {
      "parameters": {
        "jsCode": "// –ë–µ—Ä—ë–º –ò–°–•–û–î–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –∏–∑ Init Model Context (—Å–æ–¥–µ—Ä–∂–∏—Ç API key, model_id)\nconst modelContext = $('üöÄ Init Model Context').first().json;\n\n// –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ—Å—Ç–∞ (HTTP-–æ—Ç–≤–µ—Ç –æ—Ç OpenRouter)\nconst testResponse = $input.first().json;\n\n// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\nconst success = !!testResponse.choices;\nconst latency = Date.now() - modelContext.started_at;\nconst cost = testResponse.usage?.cost || 0;\nconst response = testResponse.choices?.[0]?.message?.content || '';\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É\nconsole.log(`${success ? '‚úÖ' : '‚ùå'} Test 1 (Availability): ${latency}ms | Cost: $${cost.toFixed(6)}`);\nif (success) console.log(`   Response: \"${response.substring(0, 30)}${response.length > 30 ? '...' : ''}\"`);\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º\nreturn [{\n  json: {\n    ...modelContext, // ‚Üê —Å–æ—Ö—Ä–∞–Ω—è–µ–º –í–ï–°–¨ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–ª—é—á–∞—è API key!\n    test_results: {\n      ...modelContext.test_results, // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n      availability: { // –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞\n        success,\n        latency,\n        cost,\n        response: response.substring(0, 200)\n      }\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        720,
        256
      ],
      "id": "f1cc5198-bbe2-4814-abd2-62875176af49",
      "name": "Save_test_1_result"
    },
    {
      "parameters": {
        "jsCode": "// –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Save —É–∑–ª–∞\nconst modelContext = $('Save_test_1_result').first().json;\n\n// –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ—Å—Ç–∞ (HTTP-–æ—Ç–≤–µ—Ç –æ—Ç OpenRouter)\nconst testResponse = $input.first().json;\n\n// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\nconst success = !!testResponse.choices;\nconst latency = Date.now() - modelContext.started_at;\nconst cost = testResponse.usage?.cost || 0;\nconst response = testResponse.choices?.[0]?.message?.content || '';\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É\nconsole.log(`${success ? '‚úÖ' : '‚ùå'} Test 2 (Security): ${latency}ms | Cost: $${cost.toFixed(6)}`);\nif (success) console.log(`   Response: \"${response.substring(0, 30)}${response.length > 30 ? '...' : ''}\"`);\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º\nreturn [{\n  json: {\n    ...modelContext, // ‚Üê —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–ª—é—á–∞—è API key –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–µ—Å—Ç—ã\n    test_results: {\n      ...modelContext.test_results, // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n      security: { // –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞\n        success,\n        latency,\n        cost,\n        response: response.substring(0, 200)\n      }\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1008,
        256
      ],
      "id": "67b145be-4ea7-4d14-a20a-df480c76ac13",
      "name": "Save_test_2_result"
    },
    {
      "parameters": {
        "jsCode": "// –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Save —É–∑–ª–∞\nconst modelContext = $('Save_test_2_result').first().json;\n\n// –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ—Å—Ç–∞ (HTTP-–æ—Ç–≤–µ—Ç –æ—Ç OpenRouter)\nconst testResponse = $input.first().json;\n\n// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\nconst success = !!testResponse.choices;\nconst latency = Date.now() - modelContext.started_at;\nconst cost = testResponse.usage?.cost || 0;\nconst response = testResponse.choices?.[0]?.message?.content || '';\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É\nconsole.log(`${success ? '‚úÖ' : '‚ùå'} Test 3 (Code Quality): ${latency}ms | Cost: $${cost.toFixed(6)}`);\nif (success) console.log(`   Response: \"${response.substring(0, 30)}${response.length > 30 ? '...' : ''}\"`);\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º\nreturn [{\n  json: {\n    ...modelContext, // ‚Üê —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–ª—é—á–∞—è API key –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–µ—Å—Ç—ã\n    test_results: {\n      ...modelContext.test_results, // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n      code_quality: { // –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞\n        success,\n        latency,\n        cost,\n        response: response.substring(0, 200)\n      }\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1232,
        256
      ],
      "id": "a7d56d37-61c7-4a4a-8884-c5b118c69482",
      "name": "Save_test_3_result"
    },
    {
      "parameters": {
        "jsCode": "// –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Save —É–∑–ª–∞\nconst modelContext = $('Save_test_3_result').first().json;\n\n// –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ—Å—Ç–∞ (HTTP-–æ—Ç–≤–µ—Ç –æ—Ç OpenRouter)\nconst testResponse = $input.first().json;\n\n// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\nconst success = !!testResponse.choices;\nconst latency = Date.now() - modelContext.started_at;\nconst cost = testResponse.usage?.cost || 0;\nconst response = testResponse.choices?.[0]?.message?.content || '';\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É\nconsole.log(`${success ? '‚úÖ' : '‚ùå'} Test 4 (Reasoning): ${latency}ms | Cost: $${cost.toFixed(6)}`);\nif (success) console.log(`   Response: \"${response.substring(0, 30)}${response.length > 30 ? '...' : ''}\"`);\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º\nreturn [{\n  json: {\n    ...modelContext, // ‚Üê —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–ª—é—á–∞—è API key –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–µ—Å—Ç—ã\n    test_results: {\n      ...modelContext.test_results, // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n      reasoning: { // –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞\n        success,\n        latency,\n        cost,\n        response: response.substring(0, 200)\n      }\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1456,
        256
      ],
      "id": "3db8882d-1ce3-4ca8-bbd4-364e498af91b",
      "name": "Save_test_4_result"
    },
    {
      "parameters": {
        "jsCode": "// –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Save —É–∑–ª–∞\nconst modelContext = $('Save_test_4_result').first().json;\n\n// –ë–µ—Ä—ë–º –æ—Ç–≤–µ—Ç –æ—Ç —Ç–µ—Å—Ç–∞ (HTTP-–æ—Ç–≤–µ—Ç –æ—Ç OpenRouter)\nconst testResponse = $input.first().json;\n\n// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\nconst success = !!testResponse.choices;\nconst latency = Date.now() - modelContext.started_at;\nconst cost = testResponse.usage?.cost || 0;\nconst response = testResponse.choices?.[0]?.message?.content || '';\n\n// –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–∫—É\nconsole.log(`${success ? '‚úÖ' : '‚ùå'} Test 5 (Documentation): ${latency}ms | Cost: $${cost.toFixed(6)}`);\nif (success) console.log(`   Response: \"${response.substring(0, 30)}${response.length > 30 ? '...' : ''}\"`);\n\n// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –û–ë–ù–û–í–õ–Å–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º\nreturn [{\n  json: {\n    ...modelContext, // ‚Üê —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–ª—é—á–∞—è API key –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–µ—Å—Ç—ã\n    test_results: {\n      ...modelContext.test_results, // —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n      documentation: { // –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞\n        success,\n        latency,\n        cost,\n        response: response.substring(0, 200)\n      }\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1680,
        256
      ],
      "id": "489b1258-2415-405f-ac78-9d113f62eef2",
      "name": "Save_test_5_result"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        1856,
        64
      ],
      "id": "e2521b70-45fe-46e1-887a-7de2fa6e7b26",
      "name": "Collect All Results"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.convertToFile",
      "typeVersion": 1.1,
      "position": [
        2032,
        576
      ],
      "id": "2852121d-3238-4af8-95a3-462a7e1a1d68",
      "name": "Convert to File"
    }
  ],
  "pinData": {
    "üîÄ Merge Model Lists": [
      {
        "json": {
          "model_id": "deepseek/deepseek-chat",
          "source": "default_russia_list",
          "priority": "high",
          "region": "russia_compatible",
          "original_index": 2
        }
      },
      {
        "json": {
          "model_id": "deepseek/deepseek-chat-v3.1",
          "source": "default_russia_list",
          "priority": "high",
          "region": "russia_compatible",
          "original_index": 3
        }
      },
      {
        "json": {
          "model_id": "llama-3.1-70b-versatile",
          "source": "default_russia_list",
          "priority": "high",
          "region": "russia_compatible",
          "original_index": 4
        }
      },
      {
        "json": {
          "model_id": "llama-3.1-8b-instant",
          "source": "default_russia_list",
          "priority": "high",
          "region": "russia_compatible",
          "original_index": 5
        }
      },
      {
        "json": {
          "model_id": "upstage/solar-pro-3:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 6
        }
      },
      {
        "json": {
          "model_id": "nvidia/llama-3.1-nemotron-70b-instruct",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 7
        }
      },
      {
        "json": {
          "model_id": "arcee-ai/trinity-large-preview:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 8
        }
      },
      {
        "json": {
          "model_id": "qwen/qwen-2.5-coder-32b-instruct:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 9
        }
      },
      {
        "json": {
          "model_id": "deepseek/deepseek-coder",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 10
        }
      },
      {
        "json": {
          "model_id": "meta-llama/llama-3.1-70b-instruct:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 11
        }
      },
      {
        "json": {
          "model_id": "meta-llama/llama-3.2-3b-instruct:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 12
        }
      },
      {
        "json": {
          "model_id": "meta-llama/llama-3.2-1b-instruct:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 13
        }
      },
      {
        "json": {
          "model_id": "google/gemma-2-9b-it:free",
          "source": "default_russia_list",
          "priority": "medium",
          "region": "russia_compatible",
          "original_index": 14
        }
      },
      {
        "json": {
          "model_id": "qwen/qwen3-next-80b-a3b-instruct:free",
          "context_length": 262144,
          "description": "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized fo",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "qwen/qwen3-coder:free",
          "context_length": 262000,
          "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the ",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "stepfun/step-3.5-flash:free",
          "context_length": 256000,
          "description": "Step 3.5 Flash is StepFun's most capable open-source foundation model. Built on a sparse Mixture of ",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "nvidia/nemotron-3-nano-30b-a3b:free",
          "context_length": 256000,
          "description": "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and acc",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "deepseek/deepseek-r1-0528:free",
          "context_length": 163840,
          "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "arcee-ai/trinity-mini:free",
          "context_length": 131072,
          "description": "Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 e",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "openai/gpt-oss-120b:free",
          "context_length": 131072,
          "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI d",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "openai/gpt-oss-20b:free",
          "context_length": 131072,
          "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. I",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "z-ai/glm-4.5-air:free",
          "context_length": 131072,
          "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for a",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "google/gemma-3-27b-it:free",
          "context_length": 131072,
          "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "meta-llama/llama-3.2-3b-instruct:free",
          "context_length": 131072,
          "description": "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natu",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "nousresearch/hermes-3-llama-3.1-405b:free",
          "context_length": 131072,
          "description": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced age",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "arcee-ai/trinity-large-preview:free",
          "context_length": 131000,
          "description": "Trinity-Large-Preview is a frontier-scale open-weight language model from Arcee, built as a 400B-par",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0",
            "request": "0",
            "image": "0",
            "web_search": "0",
            "internal_reasoning": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "upstage/solar-pro-3:free",
          "context_length": 128000,
          "description": "Solar Pro 3 is Upstage's powerful Mixture-of-Experts (MoE) language model. With 102B total parameter",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "nvidia/nemotron-nano-12b-v2-vl:free",
          "context_length": 128000,
          "description": "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for vid",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text+image+video->text"
        }
      },
      {
        "json": {
          "model_id": "nvidia/nemotron-nano-9b-v2:free",
          "context_length": 128000,
          "description": "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "mistralai/mistral-small-3.1-24b-instruct:free",
          "context_length": 128000,
          "description": "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billio",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "meta-llama/llama-3.3-70b-instruct:free",
          "context_length": 128000,
          "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned gen",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "qwen/qwen3-4b:free",
          "context_length": 40960,
          "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support bo",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "liquid/lfm-2.5-1.2b-thinking:free",
          "context_length": 32768,
          "description": "LFM2.5-1.2B-Thinking is a lightweight reasoning-focused model optimized for agentic tasks, data extr",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "liquid/lfm-2.5-1.2b-instruct:free",
          "context_length": 32768,
          "description": "LFM2.5-1.2B-Instruct is a compact, high-performance instruction-tuned model built for fast on-device",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
          "context_length": 32768,
          "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-In",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "google/gemma-3-4b-it:free",
          "context_length": 32768,
          "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "google/gemma-3-12b-it:free",
          "context_length": 32768,
          "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "google/gemma-3n-e2b-it:free",
          "context_length": 8192,
          "description": "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to o",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "google/gemma-3n-e4b-it:free",
          "context_length": 8192,
          "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as pho",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0",
            "completion": "0"
          },
          "architecture": "text->text"
        }
      },
      {
        "json": {
          "model_id": "x-ai/grok-4.1-fast",
          "context_length": 2000000,
          "description": "Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like cust",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0.0000002",
            "completion": "0.0000005",
            "web_search": "0.005",
            "input_cache_read": "0.00000005"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "x-ai/grok-4-fast",
          "context_length": 2000000,
          "description": "Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0.0000002",
            "completion": "0.0000005",
            "web_search": "0.005",
            "input_cache_read": "0.00000005"
          },
          "architecture": "text+image->text"
        }
      },
      {
        "json": {
          "model_id": "openrouter/auto",
          "context_length": 2000000,
          "description": "Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), opt",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "-1",
            "completion": "-1"
          },
          "architecture": "text+image+file+audio+video->text+image"
        }
      },
      {
        "json": {
          "model_id": "google/gemini-3-flash-preview",
          "context_length": 1048576,
          "description": "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, mu",
          "source": "openrouter_catalog",
          "pricing": {
            "prompt": "0.0000005",
            "completion": "0.000003",
            "image": "0.0000005",
            "audio": "0.000001",
            "internal_reasoning": "0.000003",
            "input_cache_read": "0.00000005",
            "input_cache_write": "0.00000008333333333333334"
          },
          "architecture": "text+image+file+audio+video->text"
        }
      }
    ]
  },
  "connections": {
    "‚ñ∂Ô∏è Start Workflow": {
      "main": [
        [
          {
            "node": "‚öôÔ∏è Configuration",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "‚öôÔ∏è Configuration": {
      "main": [
        [
          {
            "node": "üìã Model Selection Logic",
            "type": "main",
            "index": 0
          },
          {
            "node": "üì° Fetch OpenRouter Catalog",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üìã Model Selection Logic": {
      "main": [
        [
          {
            "node": "üîÄ Merge Model Lists",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üì° Fetch OpenRouter Catalog": {
      "main": [
        [
          {
            "node": "üîç Filter Free Compatible Models",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üîç Filter Free Compatible Models": {
      "main": [
        [
          {
            "node": "üîÄ Merge Model Lists",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "üîÄ Merge Model Lists": {
      "main": [
        [
          {
            "node": "‚úÖ Deduplicate & Finalize List",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "‚úÖ Deduplicate & Finalize List": {
      "main": [
        [
          {
            "node": "üì¶ Split In Batches (1 item)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üì¶ Split In Batches (1 item)": {
      "main": [
        [
          {
            "node": "Collect All Results",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "üöÄ Init Model Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üöÄ Init Model Context": {
      "main": [
        [
          {
            "node": "üß™ Test 1: Availability",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üß™ Test 1: Availability": {
      "main": [
        [
          {
            "node": "Save_test_1_result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üõ°Ô∏è Test 2: Security Analysis": {
      "main": [
        [
          {
            "node": "Save_test_2_result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "‚ú® Test 3: Code Quality": {
      "main": [
        [
          {
            "node": "Save_test_3_result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üß† Test 4: Logical Reasoning": {
      "main": [
        [
          {
            "node": "Save_test_4_result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üìö Test 5: Documentation": {
      "main": [
        [
          {
            "node": "Save_test_5_result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üìä Aggregate & Rank Results (FIXED)": {
      "main": [
        [
          {
            "node": "‚úÖ Final Report & Recommendations",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save_test_1_result": {
      "main": [
        [
          {
            "node": "üõ°Ô∏è Test 2: Security Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save_test_2_result": {
      "main": [
        [
          {
            "node": "‚ú® Test 3: Code Quality",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save_test_3_result": {
      "main": [
        [
          {
            "node": "üß† Test 4: Logical Reasoning",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save_test_4_result": {
      "main": [
        [
          {
            "node": "üìö Test 5: Documentation",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save_test_5_result": {
      "main": [
        [
          {
            "node": "Collect All Results",
            "type": "main",
            "index": 1
          },
          {
            "node": "üì¶ Split In Batches (1 item)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Collect All Results": {
      "main": [
        [
          {
            "node": "üìä Aggregate & Rank Results (FIXED)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "‚úÖ Final Report & Recommendations": {
      "main": [
        [
          {
            "node": "Convert to File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1",
    "binaryMode": "separate",
    "availableInMCP": false
  },
  "versionId": "c8fffb8b-91d3-4b84-9578-91a263042e0c",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "ab2f3148efabf1a50e8cbf1d56b8f18e7d3161846f871ea3c90e3e626b1c7337"
  },
  "id": "qf5sHvq0OZfUSHGl",
  "tags": []
}